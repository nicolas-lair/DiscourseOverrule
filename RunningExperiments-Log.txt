-----
General information

1. Installation of Anaconda (done 27/03/2020)
Download for Python3.7 (https://www.anaconda.com/distribution/)


2. Installation of libraries (done 27/03/2020)
On anaconda prompt run 'conda install XX'
XX = jupyter

To check the location of anaconda installed, run 'conda info -e'
To activate pip on anaconda, run 'activate <location of anaconda installed>'
pip install YY
YY = wikipedia2vec, easyesn


3. Run code 
Go to the location of the ipynb notebook by using 'cd' command
If you run python code, just run 'python <name of sourse code>'
If you want to use jupyter notebook, run 'jupyter notebook'
On the blowser automatically displayed, click the ipynb notebook you want
In the notebook, use 'Shift+Enter' to proceed lines

-----

/word2vec/ (done 29+30/03/2020) for metusalem changed to 100 d dataset

##### cossim_metusalem2012.py #####
-> calculate and draw cosine similarity trajectory to test as well as matusalem(2012)
- input (in the sourse code)
discourse_words_1: list of discourse words which are near the target words (partial context)
discourse_words_1and2: list of all discourse words (whole context)
target_word1: expected word
target_word2: word that fit with the discourse but do not fit in the sentence
target_word3: word that dont fit at all
fig_name: figure which shows cosine similarity score between
- output
Step1: cosine similarity between discourse_words_1 and target words
Step2: cosine similarity between discourse_words_1and2 and target words

##### draw_cossim_trajectory.py #####
-> draw cosine similarity trajectory of average vector on each word
- input (in the sourse code)
subject_word: subject word for the story
target_word_false: inconsistent word for the story
target_word_true: consistent word for the story
discourse_words_with_subject: list of discourse words
fig_name: name of figure which shows the cosine similarity trajectory
- output
Step1: cosine similarity between subject and target words
Step5: cosine similarity trajectory

##### narrative_link.py (for pepper experiment) #####
-> generate narrative trajectory based on cosine similarity
- input (in the sourse code)
csv file of the similarity matrix
- output
narrative trajectory

##### similarity_matrix.py (for pepper experiment) #####
-> generate cosine similarity matrix by using word2vec
- input
two dim of list of h5w discourse words
- output
csv file of similarity matrix
figure of heatmap of the similarity matrix

-----

/reservoir/

All python code and jupyter notebook does not need for the experiment (these are just tests)

-----

/reservoir/wikiextractor-master

cirrus-extract.py
-> this is not used for the experiment (this is for Wikipedia Cirrus dump)

##### WikiExtractor.py #####
-> extract Wikipedia data from the Wikipedia dump
run 'python WikiExtractor.py  <bz2 file of Wikipedia dump> -o extracted'
- output
articles sentences of Wikipedia in 'extracted' file folder

##### wikipediaTraining_predictAverageVector.py #####
-> convert the extracted Wikipedia data into word embedding vector (word2vec), and generate input/output data for training to predict the average vector
- input
wiki2vec: pkl file of wikipedia2vec
vectorDim: dimention of word2vec (same as the pkl file)
saveDataStep: number of step for saving file of training data
maxParagraphNum: nubmer of paragraph for generating training data
- output
input/output data for training to predict the average vector

##### wikipediaTraining_predictNextWord.py #####
-> convert the extracted Wikipedia data into word embedding vector (word2vec), and generate input/output data for training to predict the next word vector
- input
wiki2vec: pkl file of wikipedia2vec
vectorDim: dimention of word2vec (same as the pkl file)
saveDataStep: number of step for saving file of training data
maxParagraphNum: nubmer of paragraph for generating training data
- output
input/output data for training to predict the next word vector

##### reservoirAveraging_vs_realAveraging.ipynb #####
pfd- this requires cupy, and does not run 6/4/2020
pfd - doenst need cupy, comment this line and it works - 3 min for training

-> compare between predicted average vector and true averaged vector
- input
vectorDim: dimention of word2vec
numNode: number of node of reservoir
inputDataTraining: input data for training generated by <wikipediaTraining_predictAverageVector.py>
outputDataTraining: output data for training generated by <wikipediaTraining_predictAverageVector.py>
wiki2vec: pkl file of wikipedia2vec
discourse_words: list of discourse words
- output
figure and values of accuracy trajectory by calculating cosine similarity

##### test.ipynb #####
-> train and test the reservoir for predicting next word vector task
- input
vectorDim: dimention of word2vec
numNode: number of node of reservoir
inputDataTraining: input data for training generated by <wikipediaTraining_predictAverageVector.py>
outputDataTraining: output data for training generated by <wikipediaTraining_predictAverageVector.py>
wiki2vec: pkl file of wikipedia2vec
discourse_words: list of discourse words
- output
figure and values of cosine similarity trajectory for predicting next word vector task

##### test_averaging.ipynb #####
-> train and test the reservoir for predicting evarage vector task
- input
vectorDim: dimention of word2vec
numNode: number of node of reservoir
inputDataTraining: input data for training generated by <wikipediaTraining_predictAverageVector.py>
outputDataTraining: output data for training generated by <wikipediaTraining_predictAverageVector.py>
wiki2vec: pkl file of wikipedia2vec
discourse_words: list of discourse words
- output
figure and values of cosine similarity trajectory for predicting evarage vector task

-----



Attention:
- Some codes need to use prepared models of word2vec, and they have very large data sizes.
- Training data files (i.e., /reservoir/wikiextractor-master/extracted, text) are not necessary if you want to do preprocess for training data (they are too large data size).
- Some codes related to reservoir need 'cupy' library, but instruction about this is not in the document because I have to check the compatible version and this information is in the white big computer in Marey. I'll try to find out it and add the explanations. The library is necessary mainly for generating training data, so if you just simulate or test the reservoir, you do not need care that.



April 12, 2020
new file
(base) C:\Users\PeterDell\Google Drive\GoogleWIP\People\Uchida\ExpDuplication\word2vec>python Federmeier1999.py
Data:
target_word_1: football
target_word_2: baseball
target_word_3: monopoly
discourse_words_1:
['enjoyed', 'good', 'game']
discourse_words_1and2:
['caught', 'pass', 'scored', 'touchdown', 'enjoyed', 'good', 'game']

Step1:
cos(discourse_vector_1, football)=0.443264
cos(discourse_vector_1, baseball)=0.389347
cos(discourse_vector_1, monopoly)=0.512583

Step2:
cos(discourse_vector_1and2, football)=0.497327
cos(discourse_vector_1and2, baseball)=0.395101
cos(discourse_vector_1and2, monopoly)=0.398740

April 13, 2020 Monday - Federmeier-Reservoir-Test.ipynb
Great day in science! wiki 

without context:
cos(discourse_vector_1, football)=0.462915
cos(discourse_vector_1, baseball)=0.458443
cos(discourse_vector_1, monopoly)=0.577933


cos(discourse_vector_1, football)=0.510154
cos(discourse_vector_1, baseball)=0.509302
cos(discourse_vector_1, monopoly)=0.526126

Now testing Chwilla1995 - word pairs - excellent resut!

1. vein blood
2. baker bread
3. leg arm
4. dark light 
5. building flat
6. insane crazy
7, storm wind
8. left right
9. sailor ship
10. leader boss
11. ball round
12. day night
13. Coast sea
14. Bed sheet
15. long short
16. shark fish
17. glory fame
18. animal beast
19. green grass
20. law justice
21. village town
22. toss throw
23. threshold door
24. alive dead
25. star sky
26. repent regret
27. answer question
28. chilly cold
29. nice sweet
30. dog cat
31. berry fruit
32. empty full
33. male female
34. fever ill
35. fault wrong
36. with together
37. trembling shaking
38. leek vegetables
39. paper pen
40. window pane

unrelated
1.	sweat text
2.	stove sports
3.	dust safe
4.	scarf ground
5.	hungry bush
6.	revenge lip
7.	rig wall
8.	eat mail
9.	target belly
10.	hose failure
11.	youth battery
12.	field gray
13.	tight point
14.	washing bike
15.	quarter family
16.	package roof
17.	palace monkey
18.	times child
19.	mess wolf
20.	monk stairs
21.	nation lol
22.	cake slow
23.	platform sprayer
24.	record nose
25.	panic blouse
26.	love sphere
27.	hairdresser more
28.	onion lock
29.	word bag
30.	jelly language
31.	skull taxi
32.	farmer cry
33.	rot free
34.	set press
35.	rock fit
36.	put spot
37.	mist leak
38.	sound dress
39.	box stitch
40.	kilo minute

so we have great result for word pairs, now there are two data sets for 

(short_disc, long_disc) x good_w, close_w, far_w)

metusalem and Federmeier

each of these 5 elements can be a set of elements

method: convert py to notebook
then add the set dimension