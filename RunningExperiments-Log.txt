-----
General information

1. Installation of Anaconda 
Download for Python3.7 (https://www.anaconda.com/distribution/)


2. Installation of libraries 
On anaconda prompt run 'conda install jupyter'

To check the location of anaconda installed, run 'conda info -e'
To activate pip on anaconda, run 'activate <location of anaconda installed>'
pip install wikipedia2vec
pip install easyesn

3. Download large files:

using wikipedia2vec requires the pretrained model file
enwiki_20180420_100d.pkl
which can be downloaded from
https://wikipedia2vec.github.io/wikipedia2vec/pretrained/
http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_100d.pkl.bz2

we also created training data files for the reservoir


inputDataTraining_4k_average.npy and 
outputDataTraining_4k_average.npy

these files are available for local download via google drive
https://drive.google.com/drive/folders/1aCVvjyASIFVu-f2A-EdTDOUD30ZzcGOP?usp=sharing


4. Run code 
Go to the location of the ipynb notebook by using 'cd' command
If you run python code, just run 'python <name of sourse code>'
If you want to use jupyter notebook, run 'jupyter notebook'
On the blowser automatically displayed, click the ipynb notebook you want
In the notebook, use 'Shift+Enter' to proceed lines

-----


Exp0.jpynb - this is the training of the reservoir to generate vector averages
	- jupyter notebook file - self explanatory
	- generates figure 1 - performace of reservoir on generating vector averages

Exp1-Pairs-Chwilla1995.jpynb 
	- jupyter notebook file
	- makes wikipedia2vec embeddings for words in pairs, and calculates the cosine similarity.
	- Generates figure 2 - ChwillaPairsBOXandHistogram.png

Exp2A-Peanut_discourse_average.py
	- compares target words salted and love to the incrementally growing discourse average vector
	- generates figure 3A - Peanut-Average-Trajectory.png

Exp2B-reservoir-Peanut.jpynb
	- similar to Exp2A, but now using trained reservoir to generate the discourse average vector.
	- generates figure 3B - test_averaging_reservoir.png

Exp2C-Peanut-reservoir-iterations.py
	- runs 50 seperate reservoirs - takes about 2 hours
	- generates seperate figures for each reservoir
	- generates summary data used for Figure 4 - this is printed to the screen, so you can run Exp2C-Peanut-reservoir-iterations.py >> savedata and then copy and paste the data from this log file into Exp2-Figures-Peanut-res-data_analysis.jpynb

Exp2-Figures-Peanut-res-data_analysis.jpynb
	- you can paste the summary data from running Exp2C and generate different statistics and figures.  you should manually remove the array refernces 
	- [array([0.480 becomes [[0.480 etc.

Exp3A-metusalem2012_average.py
	discourse_words_1: list of discourse words which are near the target words (partial context)
	discourse_words_1and2: list of all discourse words (whole context)
	target_word1: expected word
	target_word2: word that fit with the discourse but do not fit in the sentence
	target_word3: word that dont fit at all
	- output
	Step1: cosine similarity between discourse_words_1 and target words
	Step2: cosine similarity between discourse_words_1and2 and target words


Exp3B-metusalem2012_Reservoir
	- Same as 3A but using Discourse-Reservoir to accumulate the average vector
	- generates summary figure 6 from article

Exp3C-metusalem2012_Reseroir50
	- runs the 72 metusalem scenarios with 50 reservoirs
	- generates summary figure
	- also prints data that can be saved from the screen or log file (using >>) and used to generate figures in Exp3-Figures

Exp3-Figures-MetusalemResPopulationsAnalysis
	- copy paste data from screen output and make additional figures







